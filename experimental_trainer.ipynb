{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4ec14f-407b-4d17-b0af-9c8274f3602b",
   "metadata": {},
   "source": [
    "# Notebook for experimenting with different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ad5cc-6499-44e3-a5b1-05894c305cea",
   "metadata": {},
   "source": [
    "Setup the model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775d212-7bee-49d7-ae0b-303515e51705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import CoreAudioML.miscfuncs as miscfuncs\n",
    "import CoreAudioML.training as training\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from CoreAudioML.dataset import DataSet\n",
    "from CoreAudioML.networks import load_model, SimpleRNN\n",
    "from colab_functions import smoothed_spectrogram, gen_smoothed_spectrogram_plot, pyplot_to_tensor\n",
    "from scipy.io.wavfile import write\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df542123-d284-411b-bcf1-144355c1d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "GPU_CHUNK = 50000  # Chunk size, set based on GPU memory MAX ~60000\n",
    "\n",
    "data_dir = \"Data/valtteri\"\n",
    "\n",
    "hidden_size = 96\n",
    "\n",
    "model = SimpleRNN(\n",
    "    input_size=1,\n",
    "    unit_type='LSTM',\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=1,\n",
    "    skip=0,\n",
    "    num_layers=1\n",
    ")\n",
    "\n",
    "dtype = torch.float32\n",
    "device = 'cuda'\n",
    "\n",
    "torch.set_default_dtype(dtype)\n",
    "torch.set_default_device(device)\n",
    "torch.cuda.set_device(0)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03436212-c179-4c7c-9010-4d6cc1e7fdb4",
   "metadata": {},
   "source": [
    "Setup up the optimizer, scheduler and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bdb958-6473-464c-958a-a0ddae3f80f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=1e-4)\n",
    "\n",
    "# TODO: test these params\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.33, patience=200,)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=False)\n",
    "# loss_functions = training.LossWrapper({'ESRPre': 0.75, 'DC': 0.25}, 'awlp', 48000)\n",
    "loss_functions = training.LossWrapper({'ESRPre': 1.0}, 'awlp', 48000)\n",
    "train_track = training.TrainTrack()\n",
    "writer = SummaryWriter(os.path.join('TensorboardData', f'LSTM-{hidden_size}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d7c628-a260-4903-aec8-59e6fdbc2358",
   "metadata": {},
   "source": [
    "Load up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbcf985-3887-41cd-98fc-2c7a4220ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet(data_dir=data_dir)\n",
    "\n",
    "dataset.create_subset('train', frame_len=24000)  # 0.5s segments\n",
    "dataset.load_file('train', 'train')  # will look for files that end in -input and -target\n",
    "\n",
    "dataset.create_subset('val')\n",
    "dataset.load_file('validate', 'val')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c378a-ade1-4e2e-98d2-606c4045d5b1",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd2149-fcf0-47fa-b15d-27a06bf87a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If training is restarting, this will ensure the previously elapsed training time is added to the total\n",
    "init_time = time.time() - start_time + train_track['total_time']*3600\n",
    "# Set network save_state flag to true, so when the save_model method is called the network weights are saved\n",
    "model.save_state = True\n",
    "patience_counter = 0\n",
    "validation_patience_limit_epoch = 0\n",
    "epochs = 2000\n",
    "save_path = \"Results/valtteri\"\n",
    "\n",
    "# This is where training happens\n",
    "# the network records the last epoch number, so if training is restarted it will start at the correct epoch number\n",
    "for epoch in tqdm(range(train_track['current_epoch'] + 1, epochs + 1)):\n",
    "    ep_st_time = time.time()\n",
    "\n",
    "    # Run 1 epoch of training,\n",
    "    epoch_loss = model.train_epoch(\n",
    "        dataset.subsets['train'].data['input'][0],\n",
    "        dataset.subsets['train'].data['target'][0],\n",
    "        loss_functions,\n",
    "        optimizer,\n",
    "        50,  # batch size\n",
    "        200,  # initial length, number of samples before backpropagation\n",
    "        1000  # number of samples that are run before updating weights\n",
    "    )\n",
    "\n",
    "    writer.add_scalar('Time/EpochTrainingTime', time.time()-ep_st_time, epoch)\n",
    "\n",
    "    # Run validation\n",
    "    if epoch % 2 == 0:\n",
    "        val_ep_st_time = time.time()\n",
    "        val_output, val_loss = model.process_data(\n",
    "            dataset.subsets['val'].data['input'][0],\n",
    "            dataset.subsets['val'].data['target'][0],\n",
    "            loss_functions,\n",
    "            GPU_CHUNK,\n",
    "        )\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < train_track['best_val_loss']:\n",
    "            #print(\"new best val loss: %f\" % val_loss.item())\n",
    "            patience_counter = 0\n",
    "            model.save_model('model_best', save_path)\n",
    "            write(\n",
    "                os.path.join(save_path, \"best_val_out.wav\"),\n",
    "                dataset.subsets['val'].fs,\n",
    "                val_output.cpu().numpy()[:, 0, 0]\n",
    "            )\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        train_track.val_epoch_update(val_loss.item(), val_ep_st_time, time.time())\n",
    "        writer.add_scalar(\n",
    "            'TrainingAndValidation/ValidationLoss',\n",
    "            train_track['validation_losses'][-1],\n",
    "            epoch\n",
    "        )\n",
    "\n",
    "    #print('current learning rate: ' + str(optimiser.param_groups[0]['lr']))\n",
    "    train_track.train_epoch_update(epoch_loss.item(), ep_st_time, time.time(), init_time, epoch)\n",
    "    # write loss to the tensorboard (just for recording purposes)\n",
    "    writer.add_scalar('TrainingAndValidation/TrainingLoss', train_track['training_losses'][-1], epoch)\n",
    "    writer.add_scalar('TrainingAndValidation/LearningRate', optimizer.param_groups[0]['lr'], epoch)\n",
    "    model.save_model('model', save_path)\n",
    "    miscfuncs.json_save(train_track, 'training_stats', save_path)\n",
    "\n",
    "    if patience_counter > 25:\n",
    "        validation_patience_limit_epoch = epoch\n",
    "        break\n",
    "\n",
    "if validation_patience_limit_epoch:\n",
    "    print('validation patience limit reached at epoch ' + str(validation_patience_limit_epoch))\n",
    "\n",
    "print(\"done training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d71d6d8-d1b8-485e-a7fd-63a075a67735",
   "metadata": {},
   "source": [
    "Cleanup CUDA stuff and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34816b83-323f-4b19-8f86-d3319cf70fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_max_memory_allocated = torch.cuda.max_memory_allocated()\n",
    "train_track['maxmemusage'] = cuda_max_memory_allocated\n",
    "with open(os.path.join(save_path, 'maxmemusage.txt'), 'w') as f:\n",
    "    f.write(str(cuda_max_memory_allocated))\n",
    "\n",
    "# Remove dataset from memory\n",
    "del dataset\n",
    "# Empty the CUDA Cache\n",
    "torch.cuda.empty_cache()\n",
    "# Invoke garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4a6aef-2514-4337-8745-f7fff3fdf3ea",
   "metadata": {},
   "source": [
    "Create new dataset object for validation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e876f-7bec-4022-90fa-8e3e6ba10e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data set\n",
    "dataset = DataSet(data_dir=data_dir)\n",
    "# Then load the Test data set\n",
    "dataset.create_subset('test')\n",
    "dataset.load_file('test', 'test')\n",
    "\n",
    "# Loss functions to be used in test Dataset\n",
    "lossESR = training.ESRLoss()\n",
    "#lossDC = training.DCLoss()\n",
    "#lossLOGCOSH = auraloss_adapter(LogCoshLoss())\n",
    "#lossSTFT = auraloss_adapter(STFTLoss())\n",
    "#lossMRSTFT = auraloss_adapter(MultiResolutionSTFTLoss())\n",
    "\n",
    "f, y1, min_, max_ = smoothed_spectrogram(\n",
    "    dataset.subsets['test'].data['target'][0].cpu().numpy()[:, 0, 0],\n",
    "    fs=dataset.subsets['test'].fs,\n",
    "    size=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d68eab5-df64-4719-b621-1df171f9eb91",
   "metadata": {},
   "source": [
    "Test the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b38ed-1940-4534-bcc8-65991c86c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"testing the final model\")\n",
    "# Test the model the training ended with\n",
    "test_output, test_loss = model.process_data(\n",
    "    dataset.subsets['test'].data['input'][0],\n",
    "    dataset.subsets['test'].data['target'][0],\n",
    "    loss_functions,\n",
    "    GPU_CHUNK,\n",
    ")\n",
    "\n",
    "f, y2, min_, max_ = smoothed_spectrogram(\n",
    "    test_output.cpu().numpy()[:, 0, 0],\n",
    "    fs=dataset.subsets['test'].fs,\n",
    "    size=4096\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss_ESR = lossESR(test_output, dataset.subsets['test'].data['target'][0])\n",
    "    #test_loss_ESR_p = lossESR(test_output, dataset.subsets['test'].data['target'][0], pooling=True)\n",
    "    #test_loss_DC = lossDC(test_output, dataset.subsets['test'].data['target'][0])\n",
    "    #test_loss_LOGCOSH = lossLOGCOSH(test_output, dataset.subsets['test'].data['target'][0])\n",
    "    #test_loss_STFT = lossSTFT(test_output, dataset.subsets['test'].data['target'][0])\n",
    "    #test_loss_MRSTFT = lossMRSTFT(test_output, dataset.subsets['test'].data['target'][0])\n",
    "write(os.path.join(save_path, \"test_out_final.wav\"), dataset.subsets['test'].fs, test_output.cpu().numpy()[:, 0, 0])\n",
    "#write(os.path.join(save_path, \"test_final_ESR.wav\"), dataset.subsets['test'].fs, test_loss_ESR_p.cpu().numpy()[:, 0, 0])\n",
    "writer.add_scalar('Testing/FinalTestLoss', test_loss.item())\n",
    "writer.add_scalar('Testing/FinalTestESR', test_loss_ESR.item())\n",
    "#writer.add_scalar('Testing/FinalTestDC', test_loss_DC.item())\n",
    "#writer.add_scalar('Testing/FinalTestLOGCOSH', test_loss_LOGCOSH.item())\n",
    "#writer.add_scalar('Testing/FinalTestSTFT', test_loss_STFT.item())\n",
    "#writer.add_scalar('Testing/FinalTestMRSTFT', test_loss_MRSTFT.item())\n",
    "\n",
    "writer.add_image(\n",
    "    'Testing/FinalPeakSpectrogram',\n",
    "    pyplot_to_tensor(\n",
    "        gen_smoothed_spectrogram_plot(\n",
    "            f,\n",
    "            target=y1,\n",
    "            predicted=y2,\n",
    "            title='Testing/FinalPeakSpectrogram'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "train_track['test_loss_final'] = test_loss.item()\n",
    "train_track['test_lossESR_final'] = test_loss_ESR.item()\n",
    "#train_track['test_lossDC_final'] = test_loss_DC.item()\n",
    "#train_track['test_lossLOGCOSH_final'] = test_loss_LOGCOSH.item()\n",
    "#train_track['test_lossSTFT_final'] = test_loss_STFT.item()\n",
    "#train_track['test_lossMRSTFT_final'] = test_loss_MRSTFT.item()\n",
    "\n",
    "# Add input/output reference batch to training stats\n",
    "# For input batch in case of conditioned models, we assume all params equal to 0.0\n",
    "train_track['input_batch'] = dataset.subsets['test'].data['input'][0].cpu().data.numpy()[:2048, 0, 0].tolist()\n",
    "train_track['output_batch_final'] = test_output.cpu().data.numpy()[:2048, 0, 0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc641b-b9b0-4792-ba09-5cc84e653327",
   "metadata": {},
   "source": [
    "Test the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444be685-d00e-414b-8052-b4def4288164",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"testing the best model\")\n",
    "# Test the best model\n",
    "del model\n",
    "# Invoke garbage collector\n",
    "gc.collect()\n",
    "best_val_net = miscfuncs.json_load('model_best', save_path)\n",
    "model = load_model(best_val_net)\n",
    "test_output, test_loss = model.process_data(\n",
    "    dataset.subsets['test'].data['input'][0],\n",
    "    dataset.subsets['test'].data['target'][0],\n",
    "    loss_functions,\n",
    "    GPU_CHUNK,\n",
    ")\n",
    "\n",
    "f, y2, min_, max_ = smoothed_spectrogram(test_output.cpu().numpy()[:, 0, 0], fs=dataset.subsets['test'].fs, size=4096)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss_ESR = lossESR(test_output, dataset.subsets['test'].data['target'][0])\n",
    "    #test_loss_ESR_p = lossESR(test_output, dataset.subsets['test'].data['target'][0], pooling=True)\n",
    "    #test_loss_DC = lossDC(test_output, dataset.subsets['test'].data['target'][0])\n",
    "    #test_loss_LOGCOSH = lossLOGCOSH(test_output, dataset.subsets['test'].data['target'][0])\n",
    "    #test_loss_STFT = lossSTFT(test_output, dataset.subsets['test'].data['target'][0])\n",
    "    #test_loss_MRSTFT = lossMRSTFT(test_output, dataset.subsets['test'].data['target'][0])\n",
    "write(os.path.join(save_path, \"test_out_best.wav\"), dataset.subsets['test'].fs, test_output.cpu().numpy()[:, 0, 0])\n",
    "#write(os.path.join(save_path, \"test_best_ESR.wav\"), dataset.subsets['test'].fs, test_loss_ESR_p.cpu().numpy()[:, 0, 0])\n",
    "writer.add_scalar('Testing/BestTestLoss', test_loss.item())\n",
    "writer.add_scalar('Testing/BestTestESR', test_loss_ESR.item())\n",
    "#writer.add_scalar('Testing/BestTestDC', test_loss_DC.item())\n",
    "#writer.add_scalar('Testing/BestTestLOGCOSH', test_loss_LOGCOSH.item())\n",
    "#writer.add_scalar('Testing/BestTestSTFT', test_loss_STFT.item())\n",
    "#writer.add_scalar('Testing/BestTestMRSTFT', test_loss_MRSTFT.item())\n",
    "\n",
    "writer.add_image('Testing/BestPeakSpectrogram', pyplot_to_tensor(gen_smoothed_spectrogram_plot(f, target=y1, predicted=y2, title='Testing/BestPeakSpectrogram')))\n",
    "\n",
    "train_track['test_loss_best'] = test_loss.item()\n",
    "train_track['test_lossESR_best'] = test_loss_ESR.item()\n",
    "#train_track['test_lossDC_best'] = test_loss_DC.item()\n",
    "#train_track['test_lossLOGCOSH_best'] = test_loss_LOGCOSH.item()\n",
    "#train_track['test_lossSTFT_best'] = test_loss_STFT.item()\n",
    "#train_track['test_lossMRSTFT_best'] = test_loss_MRSTFT.item()\n",
    "\n",
    "# Add output reference batch to training stats, input already entered previously\n",
    "train_track['output_batch_best'] = test_output.cpu().data.numpy()[:2048, 0, 0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e81cd7-3a29-4941-9990-851b117ee7a8",
   "metadata": {},
   "source": [
    "Save train track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786afd8d-943b-4e5d-86fd-9ab3db84e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "miscfuncs.json_save(train_track, 'training_stats', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f76288-28ce-49f6-a5a5-d501726c6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Results/valtteri/model_best.json'\n",
    "save_path = \"Results/valtteri/\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from model_utils import save_model\n",
    "\n",
    "with open(model_path) as json_file:\n",
    "    model_data = json.load(json_file)\n",
    "    try:\n",
    "        model_type = model_data['model_data']['model']\n",
    "        if model_type != \"SimpleRNN\":\n",
    "            print(\"Error! This model type is still unsupported\")\n",
    "            raise KeyError\n",
    "        input_size = model_data['model_data']['input_size']\n",
    "        num_layers = model_data['model_data']['num_layers']\n",
    "        unit_type = model_data['model_data']['unit_type']\n",
    "        hidden_size = model_data['model_data']['hidden_size']\n",
    "        skip = int(model_data['model_data']['skip']) # How many input elements are skipped\n",
    "        output_size = model_data['model_data']['output_size']\n",
    "        bias_fl = bool(model_data['model_data']['bias_fl'])\n",
    "        lin_weight = np.array(model_data['state_dict']['lin.weight'])\n",
    "        lin_bias = np.array(model_data['state_dict']['lin.bias'])\n",
    "    except KeyError:\n",
    "        print(\"Model file %s is corrupted\" % (model))\n",
    "        exit(1)\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    input_layer = keras.layers.Input(shape=(None, input_size))\n",
    "    rnn = keras.layers.LSTM(\n",
    "        hidden_size,\n",
    "        activation=None,\n",
    "        return_sequences=True,\n",
    "        recurrent_activation=None,\n",
    "        use_bias=bias_fl,\n",
    "        unit_forget_bias=False,\n",
    "    )(input_layer)\n",
    "    dense_layer = keras.layers.Dense(\n",
    "        1, \n",
    "        kernel_initializer=\"orthogonal\", \n",
    "        bias_initializer='random_normal'\n",
    "    )(rnn)\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=dense_layer)\n",
    "\n",
    "num = 0\n",
    "WVals = np.array(model_data['state_dict']['rec.weight_ih_l%d' % num])\n",
    "UVals = np.array(model_data['state_dict']['rec.weight_hh_l%d' % num])\n",
    "bias_ih_l0 =  np.array(model_data['state_dict']['rec.bias_ih_l%d' % num])\n",
    "bias_hh_l0 = np.array(model_data['state_dict']['rec.bias_hh_l%d' % num])\n",
    "\n",
    "lstm_weights = []\n",
    "lstm_weights.append(np.transpose(WVals))\n",
    "lstm_weights.append(np.transpose(UVals))\n",
    "BVals = (bias_ih_l0 + bias_hh_l0)\n",
    "lstm_weights.append(BVals)\n",
    "model.layers[1].set_weights(lstm_weights)\n",
    "\n",
    "dense_weights = []\n",
    "dense_weights.append(lin_weight.reshape(hidden_size, 1))\n",
    "dense_weights.append(lin_bias)\n",
    "model.layers[2].set_weights(dense_weights)\n",
    "\n",
    "output_model_path = \"Results/valtteri/model_best_rtneural.json\"\n",
    "save_model(\n",
    "    model,\n",
    "    output_model_path,\n",
    "    keras.layers.InputLayer,\n",
    "    skip=skip,\n",
    "    input_batch=None,\n",
    "    output_batch=None,\n",
    "    metadata=None,\n",
    "    verbose=False\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
